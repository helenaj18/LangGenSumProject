{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scrapy selenium\n",
    "!wget https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/linux64/chrome-linux64.zip\n",
    "!wget https://edgedl.me.gvt1.com/edgedl/chrome/chrome-for-testing/116.0.5845.96/linux64/chromedriver-linux64.zip\n",
    "!unzip chrome-linux64.zip\n",
    "!unzip chromedriver-linux64.zip\n",
    "!rm *.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import scrapy\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from urllib.parse import urljoin\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from twisted.internet import reactor\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.binary_location = './chrome-linux64/chrome'\n",
    "options.add_argument('--no-sandbox')\n",
    "options.add_argument('--headless')\n",
    "service = Service(executable_path='./chromedriver-linux64/chromedriver')\n",
    "\n",
    "\n",
    "# this function gets the data from the spider's callback function and\n",
    "# saves it to the out_filename (in the specified format out_filetype)\n",
    "# (no changes necessary to this function)\n",
    "def run_spider(spider, out_filename, out_filetype):\n",
    "    if os.path.exists(out_filename):\n",
    "        os.remove(out_filename)\n",
    "\n",
    "    def f(q):\n",
    "        try:\n",
    "            runner = CrawlerRunner(settings={\n",
    "                \"FEEDS\": {\n",
    "                    out_filename: {\n",
    "                        \"format\": out_filetype},\n",
    "                },\n",
    "            })\n",
    "            deferred = runner.crawl(spider)\n",
    "            deferred.addBoth(lambda _: reactor.stop())\n",
    "            reactor.run()\n",
    "            q.put(None)\n",
    "        except Exception as e:\n",
    "            q.put(e)\n",
    "\n",
    "    q = Queue()\n",
    "    p = Process(target=f, args=(q,))\n",
    "    p.start()\n",
    "    result = q.get()\n",
    "    p.join()\n",
    "    if result is not None:\n",
    "        raise result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSJSpider(scrapy.Spider):\n",
    "\n",
    "    name = \"Charlotte\" # name your spider\n",
    "\n",
    "    # list the url(s) where to start reading\n",
    "    def start_requests(self):\n",
    "        url = \"https://seekingalpha.com/earnings/earnings-call-transcripts\"\n",
    "        yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    # the callback function (called when the webpage is fetched)\n",
    "    def parse(self, response):\n",
    "        print(response.css(\"html\").get())   # prints the contents of the webpage on the console\n",
    "\n",
    "        # write the code for extracting all quotes, author and tags from the webpage (hint use a loop!)\n",
    "        # see the documentation here: https://docs.scrapy.org/en/latest/topics/selectors.html\n",
    "        # for i, quote in enumerate(response.css(\"div.quote\")):\n",
    "        #   text = quote.css(\"span.text::text\").get()[1:-1]\n",
    "        #   author = quote.css(\"small.author::text\").get()\n",
    "        #   tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "        #   yield {\n",
    "        #       \"text\": text,\n",
    "        #       \"author\": author,\n",
    "        #       \"tags\": tags,\n",
    "        #   }\n",
    "\n",
    "        # # once all the quotes are extracted from this page, extract the next page\n",
    "        # next_page = response.css(\"li.next a::attr(href)\").get()    # extract the link to the next webpage\n",
    "        # if next_page is not None:\n",
    "        #     yield response.follow(next_page, callback=self.parse) # to follow the link, ans use the same function\n",
    "\n",
    "\n",
    "run_spider(MyAllQuotesJSSpider, \"all_quotes_js.json\", \"json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
